{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from datasets import Dataset, Features\n",
    "from datasets.features import Image as DatasetImage\n",
    "from datasets.features import ClassLabel\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from PIL import Image as PILImage\n",
    "from PIL import ImageFile\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check MPS availability for M3 chip\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 200/200 [00:00<00:00, 696728.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "dataset_dir = Path('/Users/diksha/Desktop/PROJECT/ProcessedDataset')\n",
    "file_names, labels = [], []\n",
    "\n",
    "for file in tqdm(sorted(dataset_dir.glob('*/*.*')), desc=\"Loading files\"):\n",
    "    label = str(file).split('/')[-2]  # Extracts 'Fake' or 'Real'\n",
    "    labels.append(label)\n",
    "    file_names.append(str(file))\n",
    "\n",
    "# Debug: Check total files\n",
    "print(f\"Total files found: {len(file_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (200, 2)\n",
      "Label distribution:\n",
      " label\n",
      "Fake    100\n",
      "Real    100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame (no sampling since only 200 images exist)\n",
    "df = pd.DataFrame({\"image\": file_names, \"label\": labels})\n",
    "df['image'] = df['image'].astype(str)  # Ensure paths are strings\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Label distribution:\\n\", df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset with explicit features\n",
    "dataset = Dataset.from_pandas(df, features=Features({\n",
    "    'image': DatasetImage(decode=False),\n",
    "    'label': ClassLabel(names=['Real', 'Fake'])  # Automatically maps 'Real' -> 0, 'Fake' -> 1\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping (for model compatibility)\n",
    "labels_list = ['Real', 'Fake']\n",
    "label2id = {label: i for i, label in enumerate(labels_list)}\n",
    "id2label = {i: label for i, label in enumerate(labels_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need for map_label2id since ClassLabel already handles it\n",
    "dataset = dataset.train_test_split(test_size=0.4, shuffle=True)\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "model_str = \"dima806/deepfake_vs_real_image_detection\"\n",
    "processor = ViTImageProcessor.from_pretrained(model_str)\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "_train_transforms = Compose([ToTensor(), normalize])\n",
    "_val_transforms = Compose([ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(PILImage.open(image if isinstance(image, str) else image['path']).convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "    \n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(PILImage.open(image if isinstance(image, str) else image['path']).convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "train_data.set_transform(train_transforms)\n",
    "test_data.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': {'bytes': None, 'path': '/Users/diksha/Desktop/PROJECT/ProcessedDataset/Real/real_50314.jpg'}, 'label': 0, 'pixel_values': tensor([[[ 0.7569,  0.7412,  0.7255,  ...,  0.9451,  0.9451,  0.9451],\n",
      "         [ 0.7569,  0.7412,  0.7176,  ...,  0.9451,  0.9451,  0.9451],\n",
      "         [ 0.7490,  0.7412,  0.7098,  ...,  0.9451,  0.9451,  0.9451],\n",
      "         ...,\n",
      "         [-0.9608, -0.9529, -0.9529,  ...,  0.5137,  0.6157,  0.6706],\n",
      "         [-0.9294, -0.9451, -0.9529,  ...,  0.6314,  0.7255,  0.7569],\n",
      "         [-0.8745, -0.8980, -0.9294,  ...,  0.6784,  0.7490,  0.7569]],\n",
      "\n",
      "        [[ 0.7882,  0.7725,  0.7569,  ...,  0.9922,  0.9922,  0.9922],\n",
      "         [ 0.7882,  0.7725,  0.7490,  ...,  0.9922,  0.9922,  0.9922],\n",
      "         [ 0.7804,  0.7725,  0.7412,  ...,  0.9922,  0.9922,  0.9922],\n",
      "         ...,\n",
      "         [-0.9216, -0.9137, -0.9137,  ...,  0.4980,  0.6000,  0.6549],\n",
      "         [-0.8902, -0.9059, -0.9137,  ...,  0.6000,  0.6941,  0.7255],\n",
      "         [-0.8353, -0.8588, -0.8902,  ...,  0.6471,  0.7176,  0.7255]],\n",
      "\n",
      "        [[ 0.8745,  0.8588,  0.8431,  ...,  0.9922,  0.9922,  0.9922],\n",
      "         [ 0.8745,  0.8588,  0.8353,  ...,  0.9922,  0.9922,  0.9922],\n",
      "         [ 0.8667,  0.8588,  0.8275,  ...,  0.9922,  0.9922,  0.9922],\n",
      "         ...,\n",
      "         [-0.8980, -0.8902, -0.8902,  ...,  0.3961,  0.4980,  0.5529],\n",
      "         [-0.8667, -0.8824, -0.8902,  ...,  0.5059,  0.6000,  0.6314],\n",
      "         [-0.8118, -0.8353, -0.8667,  ...,  0.5529,  0.6235,  0.6314]]])}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])  # Should include 'pixel_values' as a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example['label'] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "model = ViTForImageClassification.from_pretrained(model_str, num_labels=len(labels_list)).to(device)\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 0.00M\n"
     ]
    }
   ],
   "source": [
    "# Freeze backbone for efficiency on small dataset\n",
    "for param in model.vit.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    label_ids = eval_pred.label_ids\n",
    "    accuracy = accuracy_score(label_ids, predictions)\n",
    "    f1 = f1_score(label_ids, predictions, average='weighted')\n",
    "    cm = confusion_matrix(label_ids, predictions)\n",
    "    report = classification_report(label_ids, predictions, target_names=labels_list)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./deepfake_vs_real_image_detection\",\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,  # Log more frequently for small dataset\n",
    "    learning_rate=1e-5,  # Slightly higher than 1e-6 for faster convergence\n",
    "    per_device_train_batch_size=4,  # Reduced from 16 for M3 memory\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.02,\n",
    "    warmup_steps=5,  # Reduced for small dataset\n",
    "    remove_unused_columns=False,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    fp16=False,  # MPS doesn't support fp16 well\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and Processor saved successfully!\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./model/deepfake_vs_real_image_detection\")\n",
    "processor.save_pretrained(\"./model/deepfake_vs_real_image_detection\")\n",
    "print(\"✅ Model and Processor saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
